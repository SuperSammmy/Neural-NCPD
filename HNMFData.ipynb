{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Zgu1VVOEsfUe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import functools \n",
        "import math\n",
        "import torch\n",
        "import sklearn\n",
        "import sys\n",
        "sys.path.insert(1,\"./src\")\n",
        "\n",
        "from NNCPD import NNCPD, weights_H, Energy_Loss_Tensor2, Energy_Loss_Tensor, Recon_Loss, L21_Norm, outer_product, outer_product_np, PTF, random_NNCPD, Fro_Norm\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "sys.path.insert(1,\"./src\")\n",
        "\n",
        "from NNCPD import NNCPD, weights_H, Energy_Loss_Tensor2, Energy_Loss_Tensor, Recon_Loss, L21_Norm, outer_product, outer_product_np, PTF, random_NNCPD, Fro_Norm\n",
        "from lsqnonneg_module import LsqNonneg\n",
        "from trainNNCPD import train\n",
        "#\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from writer import Writer\n",
        "\n",
        "\n",
        "import tensorly as tl\n",
        "from tensorly import unfold as tl_unfold\n",
        "from tensorly.decomposition import parafac, non_negative_parafac\n",
        "\n",
        "torch.set_default_tensor_type(torch.DoubleTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Wn_SRhS_xcud"
      },
      "outputs": [],
      "source": [
        "# Function to enerate data\n",
        "def hnmfdatagen(cs, std=0):\n",
        "    if len(cs) > 2:\n",
        "        matrices = [np.random.rand(cs[i], cs[i+1]) for i in range(len(cs)-1)]\n",
        "        return functools.reduce(np.matmul, matrices[1:], matrices[0]) + np.random.normal(0, std, [cs[0],cs[-1]])\n",
        "        # we might want to scale our generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eykp_3wECLVE"
      },
      "outputs": [],
      "source": [
        "# Zero out proportion of generated entries\n",
        "def zero_entries(X, p_miss, mecha=\"MCAR\", opt=None, p_obs=None, q=None):\n",
        "    \"\"\"\n",
        "    Generate missing values for specifics missing-data mechanism and proportion of missing values. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
        "        Data for which missing values will be simulated.\n",
        "        If a numpy array is provided, it will be converted to a pytorch tensor.\n",
        "    p_miss : float\n",
        "        Proportion of missing values to generate for variables which will have missing values.\n",
        "    mecha : str, \n",
        "            Indicates the missing-data mechanism to be used. \"MCAR\" by default, \"MAR\", \"MNAR\" or \"MNARsmask\"\n",
        "    opt: str, \n",
        "         For mecha = \"MNAR\", it indicates how the missing-data mechanism is generated: using a logistic regression (\"logistic\"), quantile censorship (\"quantile\") or logistic regression for generating a self-masked MNAR mechanism (\"selfmasked\").\n",
        "    p_obs : float\n",
        "            If mecha = \"MAR\", or mecha = \"MNAR\" with opt = \"logistic\" or \"quanti\", proportion of variables with *no* missing values that will be used for the logistic masking model.\n",
        "    q : float\n",
        "        If mecha = \"MNAR\" and opt = \"quanti\", quantile level at which the cuts should occur.\n",
        "    \n",
        "    Returns\n",
        "    ----------\n",
        "    A dictionnary containing:\n",
        "    'X_init': the initial data matrix.\n",
        "    'X_incomp': the data with the generated missing values.\n",
        "    'mask': a matrix indexing the generated missing values.s\n",
        "    \"\"\"\n",
        "    \n",
        "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
        "    if not to_torch:\n",
        "        X = X.astype(np.float32)\n",
        "        X = torch.from_numpy(X)\n",
        "    \n",
        "    if mecha == \"MAR\":\n",
        "        mask = MAR_mask(X, p_miss, p_obs).double()\n",
        "    elif mecha == \"MNAR\" and opt == \"logistic\":\n",
        "        mask = MNAR_mask_logistic(X, p_miss, p_obs).double()\n",
        "    elif mecha == \"MNAR\" and opt == \"quantile\":\n",
        "        mask = MNAR_mask_quantiles(X, p_miss, q, 1-p_obs).double()\n",
        "    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n",
        "        mask = MNAR_self_mask_logistic(X, p_miss).double()\n",
        "    else:\n",
        "        mask = (torch.rand(X.shape) < p_miss).double()\n",
        "    \n",
        "    X_nas = X.clone()\n",
        "    X_nas[mask.bool()] = 0\n",
        "    \n",
        "    return {'X_init': X.double(), 'X_incomp': X_nas.double(), 'mask': mask}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "9uoahX2R1is2",
        "outputId": "61fe5c09-cb8a-412c-ae8b-2c712928bc6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['X_init', 'X_incomp', 'mask'])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "X = hnmfdatagen([16, 8, 4, 16])\n",
        "X = zero_entries(X, p_miss=0.33333333, mecha=\"MCAR\")\n",
        "display(dict.keys(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected np.ndarray (got list)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [59], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m net \u001b[39m=\u001b[39m NNCPD([n1,\u001b[39m4\u001b[39m],[n2,\u001b[39m4\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m loss_func \u001b[39m=\u001b[39m Energy_Loss_Tensor()\n\u001b[1;32m----> 7\u001b[0m history_unsupervised \u001b[39m=\u001b[39m train(net, X[\u001b[39m'\u001b[39;49m\u001b[39mX_incomp\u001b[39;49m\u001b[39m'\u001b[39;49m], loss_func, r, epoch \u001b[39m=\u001b[39;49m \u001b[39m15000\u001b[39;49m, lr1 \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, lr2 \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m, random_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\samue\\Documents\\Neural-NCPD\\src\\trainNNCPD.py:45\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, X, loss_func, r, epoch, lr1, lr2, random_init)\u001b[0m\n\u001b[0;32m     42\u001b[0m     factors_tl \u001b[39m=\u001b[39m non_negative_parafac(np\u001b[39m.\u001b[39masarray(X), rank\u001b[39m=\u001b[39mr)\n\u001b[0;32m     44\u001b[0m A \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mfrom_numpy(factors_tl[\u001b[39m0\u001b[39m]), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 45\u001b[0m B \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39;49mfrom_numpy(factors_tl[\u001b[39m1\u001b[39;49m]), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     46\u001b[0m C \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mfrom_numpy(factors_tl[\u001b[39m2\u001b[39m]), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m configs \u001b[39m=\u001b[39m [[{} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(net\u001b[39m.\u001b[39mdepth\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)]\n",
            "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got list)"
          ]
        }
      ],
      "source": [
        "r=8\n",
        "n1,n2 = X['X_incomp'].shape\n",
        "net = NNCPD([n1,4],[n2,4],[0,0])\n",
        "\n",
        "loss_func = Energy_Loss_Tensor()\n",
        "\n",
        "history_unsupervised = train(net, X['X_incomp'], loss_func, r, epoch = 15000, lr1 = 0, lr2 = 0.1, random_init=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function Tensor.type>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "1eff767fbcdd03d1370ac65b66d6c79e7a2557fffba34885fa9cf061889225f2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
