{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zgu1VVOEsfUe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import functools \n",
        "import math\n",
        "import torch\n",
        "import sklearn\n",
        "import sys\n",
        "sys.path.insert(1,\"./src\")\n",
        "\n",
        "from NNCPD import NNCPD, weights_H, Energy_Loss_Tensor2, Energy_Loss_Tensor, Recon_Loss, L21_Norm, outer_product, outer_product_np, PTF, random_NNCPD, Fro_Norm\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "sys.path.insert(1,\"./src\")\n",
        "\n",
        "from NNCPD import NNCPD, weights_H, Energy_Loss_Tensor2, Energy_Loss_Tensor, Recon_Loss, L21_Norm, outer_product, outer_product_np, PTF, random_NNCPD, Fro_Norm\n",
        "from lsqnonneg_module import LsqNonneg\n",
        "from trainNNCPD import train\n",
        "#\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from writer import Writer\n",
        "\n",
        "\n",
        "import tensorly as tl\n",
        "from tensorly import unfold as tl_unfold\n",
        "from tensorly.decomposition import parafac, non_negative_parafac\n",
        "\n",
        "torch.set_default_tensor_type(torch.DoubleTensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wn_SRhS_xcud"
      },
      "outputs": [],
      "source": [
        "# Function to enerate data\n",
        "def hnmfdatagen(cs, std=0):\n",
        "    if len(cs) > 2:\n",
        "        matrices = [np.random.rand(cs[i], cs[i+1]) for i in range(len(cs)-1)]\n",
        "        return functools.reduce(np.matmul, matrices[1:], matrices[0]) + np.random.normal(0, std, [cs[0],cs[-1]])\n",
        "        # we might want to scale our generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eykp_3wECLVE"
      },
      "outputs": [],
      "source": [
        "# Zero out proportion of generated entries\n",
        "def zero_entries(X, p_miss, mecha=\"MCAR\", opt=None, p_obs=None, q=None):\n",
        "    \"\"\"\n",
        "    Generate missing values for specifics missing-data mechanism and proportion of missing values. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
        "        Data for which missing values will be simulated.\n",
        "        If a numpy array is provided, it will be converted to a pytorch tensor.\n",
        "    p_miss : float\n",
        "        Proportion of missing values to generate for variables which will have missing values.\n",
        "    mecha : str, \n",
        "            Indicates the missing-data mechanism to be used. \"MCAR\" by default, \"MAR\", \"MNAR\" or \"MNARsmask\"\n",
        "    opt: str, \n",
        "         For mecha = \"MNAR\", it indicates how the missing-data mechanism is generated: using a logistic regression (\"logistic\"), quantile censorship (\"quantile\") or logistic regression for generating a self-masked MNAR mechanism (\"selfmasked\").\n",
        "    p_obs : float\n",
        "            If mecha = \"MAR\", or mecha = \"MNAR\" with opt = \"logistic\" or \"quanti\", proportion of variables with *no* missing values that will be used for the logistic masking model.\n",
        "    q : float\n",
        "        If mecha = \"MNAR\" and opt = \"quanti\", quantile level at which the cuts should occur.\n",
        "    \n",
        "    Returns\n",
        "    ----------\n",
        "    A dictionnary containing:\n",
        "    'X_init': the initial data matrix.\n",
        "    'X_incomp': the data with the generated missing values.\n",
        "    'mask': a matrix indexing the generated missing values.s\n",
        "    \"\"\"\n",
        "    \n",
        "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
        "    if not to_torch:\n",
        "        X = X.astype(np.float32)\n",
        "        X = torch.from_numpy(X)\n",
        "    \n",
        "    if mecha == \"MAR\":\n",
        "        mask = MAR_mask(X, p_miss, p_obs).double()\n",
        "    elif mecha == \"MNAR\" and opt == \"logistic\":\n",
        "        mask = MNAR_mask_logistic(X, p_miss, p_obs).double()\n",
        "    elif mecha == \"MNAR\" and opt == \"quantile\":\n",
        "        mask = MNAR_mask_quantiles(X, p_miss, q, 1-p_obs).double()\n",
        "    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n",
        "        mask = MNAR_self_mask_logistic(X, p_miss).double()\n",
        "    else:\n",
        "        mask = (torch.rand(X.shape) < p_miss).double()\n",
        "    \n",
        "    X_nas = X.clone()\n",
        "    X_nas[mask.bool()] = 0\n",
        "    \n",
        "    return {'X_init': X.double(), 'X_incomp': X_nas.double(), 'mask': mask}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "9uoahX2R1is2",
        "outputId": "61fe5c09-cb8a-412c-ae8b-2c712928bc6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'X_init': tensor([[5.9197, 3.7788, 1.9471, 6.1880, 3.7028, 7.8176, 7.6761, 4.9613, 7.8366,\n",
              "          7.0022, 7.1768, 4.6472, 5.3645, 4.9568, 4.3226, 2.7710],\n",
              "         [4.5516, 2.9403, 1.5013, 4.8703, 2.7980, 5.9765, 5.8359, 3.5637, 5.8184,\n",
              "          5.3238, 5.2725, 3.6374, 4.0903, 3.6485, 3.2175, 2.0016],\n",
              "         [4.3736, 2.7115, 1.4024, 4.7192, 2.9330, 5.7551, 5.4649, 3.2569, 5.6138,\n",
              "          5.0511, 5.0265, 3.4907, 3.9077, 3.7277, 3.4112, 1.9519],\n",
              "         [5.6417, 3.5281, 1.8336, 5.8667, 3.6077, 7.4445, 7.2431, 4.6846, 7.4591,\n",
              "          6.6235, 6.8107, 4.4011, 5.0768, 4.8119, 4.2279, 2.6549],\n",
              "         [3.8131, 2.5895, 1.2941, 4.1495, 2.3901, 5.0862, 5.0790, 3.2118, 5.1688,\n",
              "          4.6532, 4.7514, 3.0944, 3.5890, 3.1524, 2.7771, 1.7924],\n",
              "         [5.8610, 3.8782, 1.9643, 6.2531, 3.6061, 7.7639, 7.7067, 4.9037, 7.7900,\n",
              "          7.0296, 7.1468, 4.6842, 5.3986, 4.7969, 4.1828, 2.7141],\n",
              "         [4.5690, 2.8380, 1.4709, 4.8694, 2.9863, 6.0038, 5.7434, 3.4949, 5.8655,\n",
              "          5.2817, 5.2780, 3.6199, 4.0706, 3.8497, 3.4707, 2.0473],\n",
              "         [5.6598, 3.5569, 1.8414, 5.9475, 3.5507, 7.4314, 7.2082, 4.5119, 7.2932,\n",
              "          6.5813, 6.6172, 4.4526, 5.0471, 4.6757, 4.1198, 2.5540],\n",
              "         [6.4229, 4.1168, 2.1070, 6.8839, 4.0503, 8.4504, 8.2090, 5.0158, 8.2722,\n",
              "          7.5155, 7.4865, 5.1300, 5.7847, 5.2588, 4.6801, 2.8609],\n",
              "         [4.4375, 2.5550, 1.3742, 4.5581, 2.9122, 5.7596, 5.3725, 3.2854, 5.4916,\n",
              "          4.9252, 4.8931, 3.4050, 3.7729, 3.7747, 3.3926, 1.9480],\n",
              "         [4.9407, 3.1185, 1.6021, 5.3366, 3.2354, 6.5041, 6.2374, 3.7394, 6.3523,\n",
              "          5.7454, 5.7107, 3.9562, 4.4386, 4.1367, 3.7506, 2.1987],\n",
              "         [3.8332, 2.4857, 1.2612, 4.1907, 2.5068, 5.0718, 4.9129, 2.9534, 5.0085,\n",
              "          4.5297, 4.5228, 3.1027, 3.5061, 3.2048, 2.9059, 1.7264],\n",
              "         [6.3687, 3.9573, 2.0525, 6.7594, 4.1895, 8.3900, 8.0477, 4.9788, 8.2809,\n",
              "          7.4042, 7.4784, 5.0301, 5.7047, 5.4363, 4.8897, 2.9111],\n",
              "         [4.7927, 3.3484, 1.6548, 5.2557, 2.8652, 6.3842, 6.4582, 4.0489, 6.4415,\n",
              "          5.8861, 5.9384, 3.9273, 4.5336, 3.7931, 3.2939, 2.2004],\n",
              "         [4.5357, 2.9121, 1.4819, 4.9756, 2.9479, 5.9689, 5.7339, 3.3469, 5.7806,\n",
              "          5.2846, 5.1803, 3.6778, 4.0916, 3.7237, 3.3954, 1.9717],\n",
              "         [5.8097, 3.5769, 1.8633, 6.1406, 3.8122, 7.6362, 7.3000, 4.5048, 7.4957,\n",
              "          6.7111, 6.7569, 4.5723, 5.1663, 4.9455, 4.4456, 2.6360]]),\n",
              " 'X_incomp': tensor([[0.0000, 3.7788, 1.9471, 0.0000, 3.7028, 7.8176, 7.6761, 0.0000, 0.0000,\n",
              "          7.0022, 0.0000, 4.6472, 5.3645, 4.9568, 4.3226, 0.0000],\n",
              "         [4.5516, 2.9403, 1.5013, 0.0000, 0.0000, 0.0000, 5.8359, 0.0000, 5.8184,\n",
              "          5.3238, 0.0000, 0.0000, 4.0903, 3.6485, 3.2175, 0.0000],\n",
              "         [4.3736, 2.7115, 1.4024, 0.0000, 2.9330, 5.7551, 5.4649, 0.0000, 0.0000,\n",
              "          5.0511, 5.0265, 3.4907, 3.9077, 3.7277, 3.4112, 1.9519],\n",
              "         [0.0000, 0.0000, 0.0000, 5.8667, 0.0000, 7.4445, 7.2431, 4.6846, 7.4591,\n",
              "          6.6235, 6.8107, 4.4011, 0.0000, 4.8119, 0.0000, 2.6549],\n",
              "         [3.8131, 2.5895, 0.0000, 4.1495, 0.0000, 5.0862, 5.0790, 3.2118, 0.0000,\n",
              "          4.6532, 4.7514, 0.0000, 3.5890, 3.1524, 2.7771, 1.7924],\n",
              "         [5.8610, 0.0000, 0.0000, 6.2531, 0.0000, 7.7639, 0.0000, 4.9037, 7.7900,\n",
              "          7.0296, 0.0000, 4.6842, 5.3986, 0.0000, 4.1828, 0.0000],\n",
              "         [0.0000, 2.8380, 0.0000, 4.8694, 2.9863, 6.0038, 5.7434, 3.4949, 5.8655,\n",
              "          5.2817, 5.2780, 3.6199, 0.0000, 3.8497, 3.4707, 0.0000],\n",
              "         [5.6598, 3.5569, 1.8414, 5.9475, 3.5507, 7.4314, 7.2082, 4.5119, 7.2932,\n",
              "          6.5813, 0.0000, 4.4526, 5.0471, 0.0000, 4.1198, 2.5540],\n",
              "         [6.4229, 4.1168, 2.1070, 6.8839, 4.0503, 8.4504, 8.2090, 5.0158, 8.2722,\n",
              "          0.0000, 7.4865, 5.1300, 0.0000, 0.0000, 4.6801, 2.8609],\n",
              "         [4.4375, 2.5550, 1.3742, 4.5581, 2.9122, 5.7596, 5.3725, 3.2854, 5.4916,\n",
              "          4.9252, 0.0000, 3.4050, 3.7729, 0.0000, 3.3926, 1.9480],\n",
              "         [0.0000, 3.1185, 1.6021, 5.3366, 0.0000, 6.5041, 0.0000, 3.7394, 0.0000,\n",
              "          5.7454, 5.7107, 0.0000, 0.0000, 4.1367, 3.7506, 2.1987],\n",
              "         [0.0000, 2.4857, 1.2612, 0.0000, 2.5068, 0.0000, 4.9129, 2.9534, 5.0085,\n",
              "          4.5297, 4.5228, 0.0000, 3.5061, 0.0000, 2.9059, 1.7264],\n",
              "         [6.3687, 3.9573, 2.0525, 0.0000, 0.0000, 8.3900, 8.0477, 0.0000, 8.2809,\n",
              "          7.4042, 0.0000, 5.0301, 5.7047, 0.0000, 4.8897, 0.0000],\n",
              "         [4.7927, 3.3484, 0.0000, 0.0000, 2.8652, 6.3842, 0.0000, 0.0000, 6.4415,\n",
              "          5.8861, 5.9384, 3.9273, 0.0000, 3.7931, 0.0000, 2.2004],\n",
              "         [4.5357, 0.0000, 1.4819, 4.9756, 2.9479, 5.9689, 5.7339, 3.3469, 0.0000,\n",
              "          5.2846, 5.1803, 0.0000, 4.0916, 3.7237, 3.3954, 0.0000],\n",
              "         [5.8097, 3.5769, 1.8633, 6.1406, 3.8122, 0.0000, 0.0000, 0.0000, 7.4957,\n",
              "          6.7111, 0.0000, 4.5723, 5.1663, 4.9455, 4.4456, 0.0000]]),\n",
              " 'mask': tensor([[1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
              "         [0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.],\n",
              "         [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
              "         [1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
              "         [1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.],\n",
              "         [0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.]])}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "X = hnmfdatagen([16, 8, 4, 16])\n",
        "X = zero_entries(X, p_miss=0.33333333, mecha=\"MCAR\")\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "You tried to access index 2 of a CP tensor.\nYou can only access index 0 and 1 of a CP tensor(corresponding respectively to the weights and factors)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [6], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m net \u001b[39m=\u001b[39m NNCPD([n1,\u001b[39m4\u001b[39m],[n2,\u001b[39m4\u001b[39m],[\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m loss_func \u001b[39m=\u001b[39m Energy_Loss_Tensor()\n\u001b[1;32m----> 7\u001b[0m history_unsupervised \u001b[39m=\u001b[39m train(net, X[\u001b[39m'\u001b[39;49m\u001b[39mX_incomp\u001b[39;49m\u001b[39m'\u001b[39;49m], loss_func, r, epoch \u001b[39m=\u001b[39;49m \u001b[39m15000\u001b[39;49m, lr1 \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, lr2 \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m, random_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\samue\\Documents\\Neural-NCPD\\src\\trainNNCPD.py:46\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, X, loss_func, r, epoch, lr1, lr2, random_init)\u001b[0m\n\u001b[0;32m     44\u001b[0m A \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(factors_tl[\u001b[39m0\u001b[39m])), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m B \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(factors_tl[\u001b[39m1\u001b[39m])), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 46\u001b[0m C \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(factors_tl[\u001b[39m2\u001b[39;49m])), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m configs \u001b[39m=\u001b[39m [[{} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(net\u001b[39m.\u001b[39mdepth\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[0;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m config_list \u001b[39min\u001b[39;00m configs:\n",
            "File \u001b[1;32mc:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorly\\cp_tensor.py:40\u001b[0m, in \u001b[0;36mCPTensor.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactors\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m---> 40\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mYou tried to access index \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m of a CP tensor.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     41\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mYou can only access index 0 and 1 of a CP tensor\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     42\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m(corresponding respectively to the weights and factors)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(index))\n",
            "\u001b[1;31mIndexError\u001b[0m: You tried to access index 2 of a CP tensor.\nYou can only access index 0 and 1 of a CP tensor(corresponding respectively to the weights and factors)"
          ]
        }
      ],
      "source": [
        "r=8\n",
        "n1,n2 = X['X_incomp'].shape\n",
        "net = NNCPD([n1,4],[n2,4],[1,1])\n",
        "\n",
        "loss_func = Energy_Loss_Tensor()\n",
        "\n",
        "history_unsupervised = train(net, X['X_incomp'], loss_func, r, epoch = 15000, lr1 = 0, lr2 = 0.1, random_init=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "1eff767fbcdd03d1370ac65b66d6c79e7a2557fffba34885fa9cf061889225f2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
